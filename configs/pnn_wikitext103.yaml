# Configuration for reproducing paper results
# Plastic Neural Networks on WikiText-103 MLM

# Model architecture
model:
  type: pnn
  vocab_size: 30522  # BERT tokenizer
  hidden_size: 768
  num_heads: 12
  intermediate_size: 2048  # Smaller than BERT's 3072
  max_length: 128
  num_steps: 4  # Recurrent refinement steps
  dropout: 0.1
  
  # Step-wise loss weights
  step_weights:
    - 0.1  # Step 1
    - 0.2  # Step 2
    - 0.3  # Step 3
    - 0.4  # Step 4

# Dataset
data:
  dataset: wikitext-103
  max_samples: 1000000  # 1M training sequences
  max_length: 128
  mask_probability: 0.15
  tokenizer: bert-base-uncased

# Training
training:
  batch_size: 384
  gradient_accumulation_steps: 3
  effective_batch_size: 1152  # 384 * 3
  epochs: 15
  
  # Optimization
  learning_rate: 3.0e-4
  weight_decay: 0.01
  warmup_steps: 500
  lr_schedule: cosine
  min_lr: 1.0e-5
  gradient_clip: 1.0
  
  # Mixed precision
  use_amp: true
  use_tf32: true  # For A100
  
  # Checkpointing
  save_every_n_steps: 500
  keep_last_n_checkpoints: 3
  checkpoint_dir: checkpoints/pnn_wikitext103

# Hardware
hardware:
  num_workers: 8
  pin_memory: true
  prefetch_factor: 4
  
# Logging
logging:
  log_interval: 100
  eval_interval: 1  # Every epoch
  use_tensorboard: true
  use_wandb: false
  
  wandb:
    project: plastic-neural-networks
    entity: null
    name: pnn-wikitext103-reproduction

# Seed for reproducibility
seed: 42

# Expected results (for verification)
expected:
  final_accuracy: 0.474  # Â± 0.003
  best_accuracy: 0.474
  parameter_count: 53700000  # ~53.7M
  training_time_minutes: 270  # ~4.5 hours on A100
